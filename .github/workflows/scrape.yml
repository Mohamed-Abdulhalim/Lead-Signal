name: Scrape

on:
  schedule:
    - cron: "0 */12 * * *"  # run every 12 hours
  workflow_dispatch:        # allow manual trigger

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y wget unzip xvfb
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" \
            | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run scraper (never fail even if scraper crashes at the end)
        run: |
          python maps.py --categories-file categories.txt --location "Cairo, Egypt" --max-places 20 --output TheResultss.csv --headless || true

      - name: Clean CSV
        run: |
          python csv_cleaner.py --in TheResultss.csv --out Cleaned.csv

      - name: Push to Supabase
        run: python supabase_push.py Cleaned.csv
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}

      - name: Upload cleaned CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: cleaned_csv
          path: Cleaned.csv
